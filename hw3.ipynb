{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3, BEE 6940 (Due By 4/13/23, 9:00PM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**:\n",
    "\n",
    "**ID**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Problem 1 asks you to calibrate the Rahmstorf (2007) semi-empirical sea-level rise model using Markov chain Monte Carlo and `Turing.jl`.\n",
    "- Problem 2 asks you to generate projections of global sea-level rise based on this model for different Representative Concentration Pathways.\n",
    "- Problem 3 asks you to compare the results of this calibration with your bootstrap estimates from Lab 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment\n",
    "\n",
    "The following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Teaching/BEE6940/hw/hw3`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using CSVFiles # load CSV data\n",
    "using DataFrames # data storage and presentation\n",
    "using Plots # plotting library\n",
    "using StatsPlots # statistical plotting\n",
    "using Distributions # statistical distribution interface\n",
    "using Turing # probabilistic programming and MCMC\n",
    "using Optim # optimization library\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems (100 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (60 points)\n",
    "\n",
    "In this problem, we will use Markov chain Monte Carlo to calibrate the [Rahmstorf (2007)](https://doi.org/10.1073/pnas.0907765106) semi-empirical sea-level rise model:\n",
    "$$\\frac{dH(t)}{dt} = \\alpha (T(t) - T_0),$$\n",
    "where $H(t)$ is global mean sea-level anomaly (in mm), $T(t)$ is the global mean surface temperature (in $^\\circ$ C), $T_0$ is the temperature (in $^\\circ$ C) when sea-level rise is in equilibrium, and $\\alpha$ is the sea-level rise sensitivity to warming. Discretizing this equation with an annual time step yields\n",
    "$$H(t+1) = H(t) + \\alpha (T(t) - T_0).$$\n",
    "\n",
    "Our goal in this problem is to calibrate this model and quantify parametric uncertainties using Markov chain Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rahmstorf_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rahmstorf_model(α, T₀, H₀, temp_data)\n",
    "    temp_effect = α .* (temp_data .- T₀)\n",
    "    slr_predict = cumsum(temp_effect) .+ H₀\n",
    "    return slr_predict\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: In data/NOAA_IPCC_RCPtempsscenarios.csv line 426 has 0 fields but 13 fields are expected. Skipping row.\n",
      "└ @ TextParse /Users/vs498/.julia/packages/TextParse/gNKVx/src/csv.jl:382\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>6×3 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Time</th><th style = \"text-align: left;\">GMSL (mm)</th><th style = \"text-align: left;\">Historical NOAA temp &amp; CNRM RCP 8.5 with respect to 20th century</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Union{Missing, Float64}\" style = \"text-align: left;\">Float64?</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1880.0</td><td style = \"text-align: right;\">-158.7</td><td style = \"text-align: right;\">-0.16</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">1881.0</td><td style = \"text-align: right;\">-153.1</td><td style = \"text-align: right;\">-0.1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1882.0</td><td style = \"text-align: right;\">-169.9</td><td style = \"text-align: right;\">-0.12</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">1883.0</td><td style = \"text-align: right;\">-164.6</td><td style = \"text-align: right;\">-0.17</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">1884.0</td><td style = \"text-align: right;\">-143.7</td><td style = \"text-align: right;\">-0.23</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">1885.0</td><td style = \"text-align: right;\">-145.2</td><td style = \"text-align: right;\">-0.2</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Time & GMSL (mm) & Historical NOAA temp \\& CNRM RCP 8.5 with respect to 20th century\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64?\\\\\n",
       "\t\\hline\n",
       "\t1 & 1880.0 & -158.7 & -0.16 \\\\\n",
       "\t2 & 1881.0 & -153.1 & -0.1 \\\\\n",
       "\t3 & 1882.0 & -169.9 & -0.12 \\\\\n",
       "\t4 & 1883.0 & -164.6 & -0.17 \\\\\n",
       "\t5 & 1884.0 & -143.7 & -0.23 \\\\\n",
       "\t6 & 1885.0 & -145.2 & -0.2 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Time    \u001b[0m\u001b[1m GMSL (mm) \u001b[0m\u001b[1m Historical NOAA temp & CNRM RCP 8.5 with respect to\u001b[0m ⋯\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Union{Missing, Float64}                            \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  1880.0     -158.7                                                      ⋯\n",
       "   2 │  1881.0     -153.1\n",
       "   3 │  1882.0     -169.9\n",
       "   4 │  1883.0     -164.6\n",
       "   5 │  1884.0     -143.7                                                      ⋯\n",
       "   6 │  1885.0     -145.2\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data files\n",
    "slr_data = DataFrame(load(\"data/CSIRO_Recons_gmsl_yr_2015.csv\"))\n",
    "gmt_data = DataFrame(load(\"data/NOAA_IPCC_RCPtempsscenarios.csv\"))\n",
    "\n",
    "slr_data[:, :Time] = slr_data[:, :Time] .- 0.5; # remove 0.5 from Times\n",
    "dat = leftjoin(slr_data, gmt_data, on=\"Time\") # join data frames on time\n",
    "\n",
    "select!(dat, [1, 2, 6])  # drop columns we don't need\n",
    "first(dat, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr_data = dat[:, 2];\n",
    "temp_data = dat[:,3];"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify the statistical model. Denote the residuals as $$z(\\mathbf{x}, t)=y_t - F(\\mathbf{x}, t),$$ where $y_t$ is the data at time $t$ and $F(\\mathbf{x}, t)$ is the model output at time $t$ with parameters $\\mathbf{x}$. \n",
    "\n",
    "For (relative) simplicity, we will assume $z(\\mathbf{x}, t)$ is given as an AR(1) process, *i.e.* $$z(\\mathbf{x}, t) = \\rho z(\\mathbf{x}, t-1) + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\text{Normal}(0, \\sigma^2).$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unconditional variance of an AR(1) process is $$\\text{Var}(z) = \\frac{\\sigma^2}{1-\\rho^2},$$ so the likelihood for $z$ (abusing notation to only refer to the time-index, not the parameters) can be written as:\n",
    "$$\\begin{align*}\n",
    "z_1 &\\sim \\text{Normal}\\left(0, \\frac{\\sigma^2}{1-\\rho^2}\\right) \\\\\n",
    "z_t &\\sim \\text{Normal}(\\rho z_{t-1}, \\sigma^2)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1.1 (15 points)\n",
    "\n",
    "Select priors for each of the parameters $\\alpha$, $T_0$, $H_0$, $\\rho$, and $\\sigma^2$. Justify these priors based on relevant considerations of prior information and model structure and regularization/informativeness. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1.2 (15 points)\n",
    "\n",
    "Code the posterior model in `Turing.jl` syntax in the below function. As a tip, do not directly model the residuals (this will increase the model dimension due to how `Turing.jl` parses the model code), but write out the residuals in terms of the data and the model output and rearrange as discussed in class. In other words, use sampling commands like:\n",
    "\n",
    "```julia\n",
    "sl_predict = rahmstorf_model(...)\n",
    "sl_data[1] ~ Normal(sl_predict[1], 2)\n",
    "```\n",
    "rather than \n",
    "```julia\n",
    "residuals = rahmstorf_model(...) .- sl_data\n",
    "residuals[1] ~ Normal(0, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Model definition seems empty, still continue.\n",
      "└ @ DynamicPPL /Users/vs498/.julia/packages/DynamicPPL/UFajj/src/compiler.jl:633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "slr_posterior (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@model function slr_posterior(sl_data, temp_data)\n",
    "    # define priors\n",
    "\n",
    "    # define likelihood\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1.3 (15 points)\n",
    "\n",
    "Draw samples from the posterior using MCMC. How did you decide how many chains to use and how many iterations to run the chain(s) for? How did you determine if your Markov chain(s) converged (include any plots and/or quantities which you used to make this assessment)?\n",
    "\n",
    "Tip: If you find poor convergence or that your sampler is taking a while, look at your model definition. You may need to reconsider your priors, or you may be able to reformulate the model to make it more efficient. Or you might just need to increase the number of iterations, because a starting point took longer to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = let\n",
    "    model = slr_posterior(slr_data, temp_data) # load the data into the model object\n",
    "    sampler = NUTS() # use the no-u-turn sampler (there are other options, but this is a good one)\n",
    "    # replace these with your own values\n",
    "    n_per_chain = 1 # number of iterations per chain\n",
    "    nchains = 1 # number of MCMC chains to run\n",
    "#    sample(model, sampler, MCMCThreads(), n_per_chain, nchains; drop_warmup=true) # uncomment for multiple threads, i.e. nchains > 1\n",
    "    sample(model, sampler, n_per_chain; drop_warmup=true) # comment out to use multiple chains\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1.4 (15 points)\n",
    "\n",
    "Make a corner/pairs plot of the posterior samples using `corner(chain[:, :, 1])` (unfortunately, this plot recipe does not work well with multiple chains, but if your chains have converged and were run long enough, they should all look roughly the same). Do the correlations between the various parameters make sense to you? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (30 points)\n",
    "\n",
    "In this problem, we will use our calibrated model above with NOAA's RCP8.5 temperature projections to look at different point estimates of the model parameters and now their associated projections compare to the posterior predictive probability distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.1 (5 points)\n",
    "\n",
    "Compute the maximum *a posteriori* (MAP) and maximum likelhood (MLE) estimates for the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.2 (15 points)\n",
    "\n",
    "Use `gmt_data` (the first column for the years and the fourth column for temperatures) to simulate sea-levels using your model for 1,000 posterior parameter samples, as well as for the MLE and MAP estimates. Plot the 95% quantiles by year for the posterior predictive samples along with the posterior median and the MAP and MLE projections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.3 (10 points)\n",
    "\n",
    "Based on your plot in Problem 2.2, discuss the potential implications for risk assessment of using the different point estimates (MAP and MLE) vs the broader posterior predictive ensemble. When would you suggest that these estimates be used or not used?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
